{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ac27d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Error loading CSV files: [Errno 2] No such file or directory: '/kaggle/input/priyadarshini/adilabad_2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Load individual year datasets with explicit year tagging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m      df_2019 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/kaggle/input/priyadarshini/adilabad_2019.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.assign(YEAR=\u001b[32m2019\u001b[39m)\n\u001b[32m     21\u001b[39m      df_2020 = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m/kaggle/input/priyadarshini/adilabad_2020.csv\u001b[39m\u001b[33m'\u001b[39m).assign(YEAR=\u001b[32m2020\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/kaggle/input/priyadarshini/adilabad_2019.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 301\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    300\u001b[39m     seq_length = \u001b[32m15\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     model, scaler, test_data, features, target = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m     \u001b[38;5;66;03m# Generate and display forecasts\u001b[39;00m\n\u001b[32m    304\u001b[39m     forecasts = make_predictions(model, scaler, test_data, features, seq_length)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(seq_length)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_evaluate\u001b[39m(seq_length=\u001b[32m15\u001b[39m):\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     train_df, test_df, features, target = \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# Preprocess and balance\u001b[39;00m\n\u001b[32m    189\u001b[39m     train_balanced, test_processed, scaler = preprocess_and_balance_data(\n\u001b[32m    190\u001b[39m         train_df, test_df, features, target)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m      df_2022 = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m/kaggle/input/priyadarshini/adilabad_2022.csv\u001b[39m\u001b[33m'\u001b[39m).assign(YEAR=\u001b[32m2022\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading CSV files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Combine data while preserving year information\u001b[39;00m\n\u001b[32m     28\u001b[39m full_df = pd.concat([df_2019, df_2020, df_2021, df_2022], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Error loading CSV files: [Errno 2] No such file or directory: '/kaggle/input/priyadarshini/adilabad_2019.csv'"
     ]
    }
   ],
   "source": [
    "#for rainfaal adilabad using transformer model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, \n",
    "                           accuracy_score, confusion_matrix, classification_report,\n",
    "                           roc_auc_score, roc_curve, auc)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# Modified version with enhanced validation checks\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    try:\n",
    "        # Load individual year datasets with explicit year tagging\n",
    "         df_2019 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\adilabad_2019.csv').assign(YEAR=2019)\n",
    "         df_2020 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\adilabad_2020.csv').assign(YEAR=2020)\n",
    "         df_2021 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\adilabad_2021.csv').assign(YEAR=2021)\n",
    "         df_2022 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\adilabad_2022.csv').assign(YEAR=2022)\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error loading CSV files: {e}\")\n",
    "\n",
    "    # Combine data while preserving year information\n",
    "    full_df = pd.concat([df_2019, df_2020, df_2021, df_2022], ignore_index=True)\n",
    "    \n",
    "    # Data quality checks\n",
    "    required_cols = ['YEAR', 'MO', 'DY', 'PRECTOTCORR']\n",
    "    missing_cols = [col for col in required_cols if col not in full_df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Convert and clean data\n",
    "    for col in required_cols:\n",
    "        full_df[col] = pd.to_numeric(full_df[col], errors='coerce')\n",
    "    full_df.dropna(subset=required_cols, inplace=True)\n",
    "\n",
    "    # Create datetime and derived temporal features\n",
    "    full_df['datetime'] = pd.to_datetime(dict(year=full_df['YEAR'], \n",
    "                                            month=full_df['MO'], \n",
    "                                            day=full_df['DY']))\n",
    "    full_df['DOY'] = full_df['datetime'].dt.dayofyear\n",
    "    full_df['sin_DOY'] = np.sin(2 * np.pi * full_df['DOY'] / 365.25)\n",
    "    full_df['cos_DOY'] = np.cos(2 * np.pi * full_df['DOY'] / 365.25)\n",
    "\n",
    "    # Create lagged precipitation features\n",
    "    for lag in [1, 3, 7]:\n",
    "        full_df[f'PRECTOTCORR_lag{lag}'] = full_df['PRECTOTCORR'].shift(lag)\n",
    "\n",
    "    # Calculate Sea Level Temperature (Adilabad elevation: 254m)\n",
    "    full_df['SLT'] = full_df['TS'] + (0.0065 * 254)\n",
    "    full_df.dropna(inplace=True)\n",
    "\n",
    "    # Define final feature set\n",
    "    features = ['SLT', 'SLP', 'T2M', 'TS', 'T2M_MAX', 'T2M_MIN', \n",
    "               'RH2M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "               'sin_DOY', 'cos_DOY', \n",
    "               'PRECTOTCORR_lag1', 'PRECTOTCORR_lag3', 'PRECTOTCORR_lag7']\n",
    "    target = 'PRECTOTCORR'\n",
    "    full_df['PRECTOTCORR_binary'] = (full_df[target] > 0).astype(int)\n",
    "\n",
    "    # Strict year-based splitting with validation\n",
    "    train_years = [2019, 2020, 2021]\n",
    "    test_year = 2022\n",
    "    \n",
    "    train_df = full_df[full_df['YEAR'].isin(train_years)].copy()\n",
    "    test_df = full_df[full_df['YEAR'] == test_year].copy()\n",
    "    \n",
    "    # Additional validation checks\n",
    "    if len(train_df) == 0:\n",
    "        raise ValueError(\"No training data found for specified years\")\n",
    "    if len(test_df) == 0:\n",
    "        raise ValueError(\"No test data found for specified year\")\n",
    "    \n",
    "    # Check for temporal leakage\n",
    "    max_train_date = train_df['datetime'].max()\n",
    "    min_test_date = test_df['datetime'].min()\n",
    "    if min_test_date <= max_train_date:\n",
    "        raise ValueError(f\"Temporal leakage detected! Latest training date ({max_train_date}) is after earliest test date ({min_test_date})\")\n",
    "\n",
    "    # Validate the split\n",
    "    print(\"=== Data Partition Validation ===\")\n",
    "    print(f\"Training years: {train_years}\")\n",
    "    print(f\"Testing year:   {test_year}\")\n",
    "    print(f\"\\nTraining period: {train_df['datetime'].min().date()} to {train_df['datetime'].max().date()}\")\n",
    "    print(f\"Testing period:  {test_df['datetime'].min().date()} to {test_df['datetime'].max().date()}\")\n",
    "    print(f\"\\nTraining samples: {len(train_df):,}\")\n",
    "    print(f\"Testing samples:  {len(test_df):,}\")\n",
    "    print(\"\\nClass distribution (Training):\")\n",
    "    print(train_df['PRECTOTCORR_binary'].value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution (Testing):\")\n",
    "    print(test_df['PRECTOTCORR_binary'].value_counts(normalize=True))\n",
    "    \n",
    "    return train_df, test_df, features, 'PRECTOTCORR_binary'\n",
    "\n",
    "# Rest of your code remains the same...\n",
    "# Step 2: Balance and Scale Data\n",
    "def preprocess_and_balance_data(train_df, test_df, features, target):\n",
    "    # Initialize scaler and scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df[features])\n",
    "    test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(train_scaled, train_df[target])\n",
    "    \n",
    "    # Create balanced DataFrames\n",
    "    train_balanced = pd.DataFrame(X_resampled, columns=features)\n",
    "    train_balanced[target] = y_resampled\n",
    "    \n",
    "    test_processed = pd.DataFrame(test_scaled, columns=features, index=test_df.index)\n",
    "    test_processed[target] = test_df[target]\n",
    "    test_processed['DOY'] = test_df['DOY']  # Preserve for sequence generation\n",
    "    \n",
    "    print(\"\\nAfter SMOTE balancing:\")\n",
    "    print(train_balanced[target].value_counts())\n",
    "    \n",
    "    return train_balanced, test_processed, scaler\n",
    "\n",
    "# Step 3: Create Time Sequences\n",
    "def create_sequences(data, features, target, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[features].iloc[i:i+seq_length].values)\n",
    "        y.append(data[target].iloc[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Build Transformer Model\n",
    "def build_transformer_model(seq_length, n_features):\n",
    "    inputs = Input(shape=(seq_length, n_features))\n",
    "    \n",
    "    # Positional Encoding\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "    positions = positions / tf.math.pow(10000, 2 * (tf.range(n_features, dtype=tf.float32) // 2) / n_features)\n",
    "    positions = tf.reshape(positions, (1, seq_length, n_features))\n",
    "    x = inputs + positions\n",
    "\n",
    "    # Transformer Block 1\n",
    "    attention_output = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "    attention_output = Dropout(0.3)(attention_output)\n",
    "    out1 = Add()([x, attention_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "    \n",
    "    # Feed Forward Network\n",
    "    ffn = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(out1)\n",
    "    ffn = Dropout(0.3)(ffn)\n",
    "    ffn = Dense(n_features, kernel_regularizer=l2(0.01))(ffn)\n",
    "    out2 = Add()([out1, ffn])\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "\n",
    "    # Transformer Block 2\n",
    "    attention_output = MultiHeadAttention(num_heads=8, key_dim=64)(out2, out2)\n",
    "    attention_output = Dropout(0.3)(attention_output)\n",
    "    out3 = Add()([out2, attention_output])\n",
    "    out3 = LayerNormalization(epsilon=1e-6)(out3)\n",
    "    \n",
    "    ffn = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(out3)\n",
    "    ffn = Dropout(0.3)(ffn)\n",
    "    ffn = Dense(n_features, kernel_regularizer=l2(0.01))(ffn)\n",
    "    out4 = Add()([out3, ffn])\n",
    "    out4 = LayerNormalization(epsilon=1e-6)(out4)\n",
    "\n",
    "    # Output Layer\n",
    "    x = Flatten()(out4)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', \n",
    "                        tf.keras.metrics.Precision(name='precision'),\n",
    "                        tf.keras.metrics.Recall(name='recall'),\n",
    "                        tf.keras.metrics.AUC(name='auc')])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Step 5: Train and Evaluate\n",
    "def train_and_evaluate(seq_length=15):\n",
    "    # Load and prepare data\n",
    "    train_df, test_df, features, target = load_and_prepare_data()\n",
    "    \n",
    "    # Preprocess and balance\n",
    "    train_balanced, test_processed, scaler = preprocess_and_balance_data(\n",
    "        train_df, test_df, features, target)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_balanced, features, target, seq_length)\n",
    "    X_test, y_test = create_sequences(test_processed, features, target, seq_length)\n",
    "    \n",
    "    print(f\"\\nFinal input shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_transformer_model(seq_length, len(features))\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                       epochs=200,\n",
    "                       batch_size=32,\n",
    "                       validation_split=0.2,\n",
    "                       callbacks=callbacks,\n",
    "                       verbose=1)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\n=== Test Set Evaluation ===\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1:        {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_prob):.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot results\n",
    "    plot_results(history, y_test, y_pred_prob, y_pred)\n",
    "    \n",
    "    return model, scaler, test_processed, features, target\n",
    "\n",
    "def plot_results(history, y_test, y_pred_prob, y_pred):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Training history\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.subplot(2, 2, 2)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc(fpr, tpr):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['No Rain', 'Rain'])\n",
    "    plt.yticks([0, 1], ['No Rain', 'Rain'])\n",
    "    \n",
    "    # Actual vs Predicted\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(y_test[:100], 'o-', label='Actual')\n",
    "    plt.plot(y_pred[:100], 'x--', label='Predicted')\n",
    "    plt.title('First 100 Test Samples')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Rain (1) or No Rain (0)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 6: Make Future Predictions\n",
    "def make_predictions(model, scaler, test_data, features, seq_length, days=7):\n",
    "    last_sequence = test_data[features].iloc[-seq_length:].values\n",
    "    predictions = []\n",
    "    last_doy = test_data['DOY'].iloc[-1]\n",
    "    \n",
    "    for _ in range(days):\n",
    "        x = last_sequence.reshape(1, seq_length, len(features))\n",
    "        pred_prob = model.predict(x, verbose=0)[0][0]\n",
    "        pred = 1 if pred_prob > 0.5 else 0\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        # Update sequence with predicted value\n",
    "        new_row = last_sequence[-1].copy()\n",
    "        new_row[-3] = pred_prob  # Update lag1 feature\n",
    "        last_sequence = np.vstack([last_sequence[1:], new_row])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seq_length = 15\n",
    "    model, scaler, test_data, features, target = train_and_evaluate(seq_length)\n",
    "    \n",
    "    # Generate and display forecasts\n",
    "    forecasts = make_predictions(model, scaler, test_data, features, seq_length)\n",
    "    print(\"\\n=== 7-Day Rainfall Forecast ===\")\n",
    "    for day, pred in enumerate(forecasts, 1):\n",
    "        print(f\"Day {day}: {'Rain' if pred == 1 else 'No Rain'}\")\n",
    "    \n",
    "    # Plot forecast\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(1, 8), forecasts, 'o-')\n",
    "    plt.title('Adilabad 7-Day Rainfall Forecast')\n",
    "    plt.xlabel('Days Ahead')\n",
    "    plt.ylabel('Rain Prediction (1=Rain)')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863c3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Rainfall venv)",
   "language": "python",
   "name": "rainfall-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
