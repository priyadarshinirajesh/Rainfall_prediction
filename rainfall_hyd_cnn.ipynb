{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff7aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Error loading CSV files: [Errno 2] No such file or directory: '/kaggle/input/rainhyd/Hyderabad_2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     df_2019 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/kaggle/input/rainhyd/Hyderabad_2019.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.assign(YEAR=\u001b[32m2019\u001b[39m)\n\u001b[32m     21\u001b[39m     df_2020 = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m/kaggle/input/rainhyd/hyderabad_2020.csv\u001b[39m\u001b[33m'\u001b[39m).assign(YEAR=\u001b[32m2020\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priya\\rainfall_prediction\\Rainfall_prediction\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/kaggle/input/rainhyd/Hyderabad_2019.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 255\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    254\u001b[39m     seq_length = \u001b[32m15\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     model, scaler, test_scaled, features, target = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m     \u001b[38;5;66;03m# Make future predictions\u001b[39;00m\n\u001b[32m    258\u001b[39m     future_preds = make_predictions(model, scaler, test_scaled, features, seq_length)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(seq_length)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_evaluate\u001b[39m(seq_length=\u001b[32m15\u001b[39m):\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     train_df, test_df, features, target = \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Balance and preprocess\u001b[39;00m\n\u001b[32m    143\u001b[39m     train_balanced, test_scaled, scaler = preprocess_and_balance_data(train_df, test_df, features, target)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m     df_2022 = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m/kaggle/input/rainhyd/hyderabad_2022.csv\u001b[39m\u001b[33m'\u001b[39m).assign(YEAR=\u001b[32m2022\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading CSV files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Concatenate data\u001b[39;00m\n\u001b[32m     28\u001b[39m full_df = pd.concat([df_2019, df_2020, df_2021, df_2022], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Error loading CSV files: [Errno 2] No such file or directory: '/kaggle/input/rainhyd/Hyderabad_2019.csv'"
     ]
    }
   ],
   "source": [
    "# code for rainfall prediction in hyderabad using cnn+lstm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, \n",
    "                           accuracy_score, confusion_matrix, classification_report,\n",
    "                           roc_auc_score, roc_curve, auc)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv1D, MaxPooling1D, LSTM, \n",
    "                                   Dense, Dropout, Flatten, BatchNormalization)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "def load_and_prepare_data():\n",
    "    try:\n",
    "        df_2019 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\hyderabad_2019.csv').assign(YEAR=2019)\n",
    "        df_2020 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\hyderabad_2020.csv').assign(YEAR=2020)\n",
    "        df_2021 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\hyderabad_2021.csv').assign(YEAR=2021)\n",
    "        df_2022 = pd.read_csv('C:\\\\Users\\\\priya\\\\main_rainfall_prediction\\\\Rainfall_prediction\\\\datatsets\\\\hyderabad_2022.csv').assign(YEAR=2022)\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error loading CSV files: {e}\")\n",
    "\n",
    "    # Concatenate data\n",
    "    full_df = pd.concat([df_2019, df_2020, df_2021, df_2022], ignore_index=True)\n",
    "    \n",
    "    # Data preprocessing\n",
    "    required_cols = ['YEAR', 'MO', 'DY', 'PRECTOTCORR']  # Include PRECTOTCORR for clarity\n",
    "    for col in required_cols:\n",
    "        full_df[col] = pd.to_numeric(full_df[col], errors='coerce')\n",
    "    full_df.dropna(subset=required_cols, inplace=True)\n",
    "\n",
    "    full_df['datetime'] = pd.to_datetime(dict(year=full_df['YEAR'], month=full_df['MO'], day=full_df['DY']))\n",
    "    full_df['DOY'] = full_df['datetime'].dt.dayofyear\n",
    "    full_df['sin_DOY'] = np.sin(2 * np.pi * full_df['DOY'] / 365.25)\n",
    "    full_df['cos_DOY'] = np.cos(2 * np.pi * full_df['DOY'] / 365.25)\n",
    "\n",
    "    # Add lagged features\n",
    "    full_df['PRECTOTCORR_lag1'] = full_df['PRECTOTCORR'].shift(1)\n",
    "    full_df['PRECTOTCORR_lag3'] = full_df['PRECTOTCORR'].shift(3)\n",
    "    full_df['PRECTOTCORR_lag7'] = full_df['PRECTOTCORR'].shift(7)\n",
    "\n",
    "    # Calculate Sea Level Temperature\n",
    "    elevation = 505.5  # Adilabad elevation in meters\n",
    "    lapse_rate = 0.0065  # Â°C per meter\n",
    "    full_df['SLT'] = full_df['TS'] + (lapse_rate * elevation)\n",
    "    full_df.dropna(inplace=True)\n",
    "\n",
    "    # Define features and target\n",
    "    features = ['SLT', 'SLP', 'T2M', 'TS', 'T2M_MAX', 'T2M_MIN', 'RH2M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "                'sin_DOY', 'cos_DOY', 'PRECTOTCORR_lag1', 'PRECTOTCORR_lag3', 'PRECTOTCORR_lag7']\n",
    "    full_df['PRECTOTCORR_binary'] = (full_df['PRECTOTCORR'] > 0).astype(int)\n",
    "    target = 'PRECTOTCORR_binary'\n",
    "\n",
    "    # Strict year-based split\n",
    "    train_df = full_df[full_df['YEAR'].isin([2019, 2020, 2021])].copy()\n",
    "    test_df = full_df[full_df['YEAR'] == 2022].copy()\n",
    "    \n",
    "    # Validate the split\n",
    "    print(\"=== Data Partition Validation ===\")\n",
    "    print(f\"Training period: {train_df['datetime'].min().date()} to {train_df['datetime'].max().date()}\")\n",
    "    print(f\"Testing period:  {test_df['datetime'].min().date()} to {test_df['datetime'].max().date()}\")\n",
    "    print(f\"Training samples: {len(train_df):,}\")\n",
    "    print(f\"Testing samples:  {len(test_df):,}\")\n",
    "    print(\"\\nClass distribution (Training):\")\n",
    "    print(train_df['PRECTOTCORR_binary'].value_counts(normalize=True))\n",
    "    \n",
    "    return train_df, test_df, features, target\n",
    "\n",
    "# Step 2: Balance and Preprocess Data\n",
    "def preprocess_and_balance_data(train_df, test_df, features, target):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df[features])\n",
    "    test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    train_balanced, train_target = smote.fit_resample(train_scaled, train_df[target])\n",
    "    \n",
    "    # Create DataFrames\n",
    "    train_balanced_df = pd.DataFrame(train_balanced, columns=features)\n",
    "    train_balanced_df[target] = train_target\n",
    "    \n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=features, index=test_df.index)\n",
    "    test_scaled_df[target] = test_df[target]\n",
    "    test_scaled_df['DOY'] = test_df['DOY']\n",
    "    \n",
    "    return train_balanced_df, test_scaled_df, scaler\n",
    "\n",
    "# Step 3: Create Sequences\n",
    "def create_sequences(data, features, target, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq_features = data[features].iloc[i:i+seq_length].values\n",
    "        X.append(seq_features)\n",
    "        y.append(data[target].iloc[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Build CNN-LSTM Model\n",
    "def build_cnn_lstm_model(seq_length, n_features):\n",
    "    model = Sequential([\n",
    "        # CNN Part\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "               input_shape=(seq_length, n_features), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # LSTM Part\n",
    "        LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', \n",
    "                          tf.keras.metrics.Precision(name='precision'),\n",
    "                          tf.keras.metrics.Recall(name='recall'),\n",
    "                          tf.keras.metrics.AUC(name='auc')])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Step 5: Train and Evaluate\n",
    "def train_and_evaluate(seq_length=15):\n",
    "    # Load and prepare data\n",
    "    train_df, test_df, features, target = load_and_prepare_data()\n",
    "    \n",
    "    # Balance and preprocess\n",
    "    train_balanced, test_scaled, scaler = preprocess_and_balance_data(train_df, test_df, features, target)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_balanced, features, target, seq_length)\n",
    "    X_test, y_test = create_sequences(test_scaled, features, target, seq_length)\n",
    "    \n",
    "    print(f\"\\nTraining shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_cnn_lstm_model(seq_length, len(features))\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=32,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=1)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_prob):.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot results\n",
    "    plot_results(history, y_test, y_pred_prob, y_pred)\n",
    "    \n",
    "    return model, scaler, test_scaled, features, target\n",
    "\n",
    "def plot_results(history, y_test, y_pred_prob, y_pred):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.subplot(2, 2, 3)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc(fpr, tpr):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.subplot(2, 2, 4)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Actual vs Predicted plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:100], label='Actual', marker='o')\n",
    "    plt.plot(y_pred[:100], label='Predicted', marker='x')\n",
    "    plt.title('Actual vs Predicted Rainfall (First 100 Samples)')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Rain (1) or No Rain (0)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "def make_predictions(model, scaler, test_scaled, features, seq_length, days=7):\n",
    "    last_sequence = test_scaled[features].iloc[-seq_length:].values\n",
    "    predictions = []\n",
    "    \n",
    "    for _ in range(days):\n",
    "        # Reshape for model input\n",
    "        x = last_sequence.reshape(1, seq_length, len(features))\n",
    "        # Predict\n",
    "        pred = model.predict(x, verbose=0)[0][0]\n",
    "        predictions.append(1 if pred > 0.5 else 0)\n",
    "        # Update sequence\n",
    "        new_row = last_sequence[-1].copy()\n",
    "        new_row[-3] = pred  # Update lag1 feature\n",
    "        last_sequence = np.vstack([last_sequence[1:], new_row])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    seq_length = 15\n",
    "    model, scaler, test_scaled, features, target = train_and_evaluate(seq_length)\n",
    "    \n",
    "    # Make future predictions\n",
    "    future_preds = make_predictions(model, scaler, test_scaled, features, seq_length)\n",
    "    print(\"\\nNext 7 Day Predictions:\")\n",
    "    for i, pred in enumerate(future_preds, 1):\n",
    "        print(f\"Day {i}: {'Rain' if pred == 1 else 'No Rain'}\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(1, 8), future_preds, 'o-')\n",
    "    plt.title('7-Day Rainfall Forecast')\n",
    "    plt.xlabel('Days Ahead')\n",
    "    plt.ylabel('Rain (1) or No Rain (0)')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba7768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Rainfall venv)",
   "language": "python",
   "name": "rainfall-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
