{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce9520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hyderabad Rainfall Prediction Model ===\n",
      "Initializing with rigorous temporal validation...\n",
      "\n",
      "ERROR: Error loading CSV files: [Errno 2] No such file or directory: '/kaggle/input/predict/Hyderabad_2019.csv'\n",
      "Model training failed due to data validation issues.\n"
     ]
    }
   ],
   "source": [
    "#for rainfall in hyderabad using transformer model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, \n",
    "                           accuracy_score, confusion_matrix, classification_report,\n",
    "                           roc_auc_score, roc_curve, auc)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enhanced version with rigorous temporal validation\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    try:\n",
    "        # Load individual year datasets with validation\n",
    "        df_2019 = pd.read_csv('/kaggle/input/predict/Hyderabad_2019.csv').assign(YEAR=2019)\n",
    "        df_2020 = pd.read_csv('/kaggle/input/predict/hyderabad_2020.csv').assign(YEAR=2020)\n",
    "        df_2021 = pd.read_csv('/kaggle/input/predict/Hyderabad_2021.csv').assign(YEAR=2021)\n",
    "        df_2022 = pd.read_csv('/kaggle/input/predict/hyderabad_2022.csv').assign(YEAR=2022)\n",
    "        \n",
    "        # Verify year consistency in each file\n",
    "        for year, df in zip([2019, 2020, 2021, 2022], [df_2019, df_2020, df_2021, df_2022]):\n",
    "            if not (df['YEAR'] == year).all():\n",
    "                raise ValueError(f\"Inconsistent year data in {year} file\")\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error loading CSV files: {e}\")\n",
    "\n",
    "    # Combine data with rigorous checks\n",
    "    full_df = pd.concat([df_2019, df_2020, df_2021, df_2022], ignore_index=True)\n",
    "    \n",
    "    # Data quality validation\n",
    "    required_cols = ['YEAR', 'MO', 'DY', 'PRECTOTCORR']\n",
    "    missing_cols = [col for col in required_cols if col not in full_df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Convert and clean data with validation\n",
    "    for col in required_cols:\n",
    "        full_df[col] = pd.to_numeric(full_df[col], errors='coerce')\n",
    "        if full_df[col].isnull().any():\n",
    "            print(f\"Warning: NaN values found in {col} after conversion\")\n",
    "    full_df.dropna(subset=required_cols, inplace=True)\n",
    "\n",
    "    # Create datetime with validation\n",
    "    full_df['datetime'] = pd.to_datetime(dict(year=full_df['YEAR'], \n",
    "                                       month=full_df['MO'], \n",
    "                                       day=full_df['DY']), errors='coerce')\n",
    "    if full_df['datetime'].isnull().any():\n",
    "        raise ValueError(\"Invalid date values found in the data\")\n",
    "    \n",
    "    # Temporal features\n",
    "    full_df['DOY'] = full_df['datetime'].dt.dayofyear\n",
    "    full_df['sin_DOY'] = np.sin(2 * np.pi * full_df['DOY'] / 365.25)\n",
    "    full_df['cos_DOY'] = np.cos(2 * np.pi * full_df['DOY'] / 365.25)\n",
    "\n",
    "    # Lag features with boundary checks\n",
    "    for lag in [1, 3, 7]:\n",
    "        full_df[f'PRECTOTCORR_lag{lag}'] = full_df.groupby('YEAR')['PRECTOTCORR'].shift(lag)\n",
    "    \n",
    "    # Hyderabad-specific elevation adjustment\n",
    "    hyderabad_elevation = 505.5  # meters\n",
    "    full_df['SLT'] = full_df['TS'] + (0.0065 * hyderabad_elevation)\n",
    "    full_df.dropna(inplace=True)\n",
    "\n",
    "    # Feature selection\n",
    "    features = ['SLT', 'SLP', 'T2M', 'TS', 'T2M_MAX', 'T2M_MIN', \n",
    "               'RH2M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "               'sin_DOY', 'cos_DOY', \n",
    "               'PRECTOTCORR_lag1', 'PRECTOTCORR_lag3', 'PRECTOTCORR_lag7']\n",
    "    target = 'PRECTOTCORR'\n",
    "    full_df['PRECTOTCORR_binary'] = (full_df[target] > 0).astype(int)\n",
    "\n",
    "    # Rigorous year-based splitting\n",
    "    train_years = {2019, 2020, 2021}  # Using set for unambiguous membership testing\n",
    "    test_year = 2022\n",
    "    \n",
    "    # Verify no overlap between train and test years\n",
    "    if test_year in train_years:\n",
    "        raise ValueError(\"Test year appears in training set!\")\n",
    "    \n",
    "    train_df = full_df[full_df['YEAR'].isin(train_years)].copy()\n",
    "    test_df = full_df[full_df['YEAR'] == test_year].copy()\n",
    "    \n",
    "    # Temporal integrity checks\n",
    "    max_train_date = train_df['datetime'].max()\n",
    "    min_test_date = test_df['datetime'].min()\n",
    "    \n",
    "    if min_test_date <= max_train_date:\n",
    "        raise ValueError(f\"Temporal leakage detected! Latest training date ({max_train_date}) >= earliest test date ({min_test_date})\")\n",
    "    \n",
    "    # Data distribution validation\n",
    "    print(\"\\n=== Rigorous Data Validation ===\")\n",
    "    print(\"Training Years:\", sorted(train_df['YEAR'].unique()))\n",
    "    print(\"Testing Year: \", test_df['YEAR'].unique()[0])\n",
    "    print(\"\\nDate Ranges:\")\n",
    "    print(f\"Training: {train_df['datetime'].min().date()} to {train_df['datetime'].max().date()}\")\n",
    "    print(f\"Testing:  {test_df['datetime'].min().date()} to {test_df['datetime'].max().date()}\")\n",
    "    print(\"\\nSample Counts:\")\n",
    "    print(f\"Training: {len(train_df):,} samples\")\n",
    "    print(f\"Testing:  {len(test_df):,} samples\")\n",
    "    \n",
    "    # Verify no date overlaps\n",
    "    train_dates = set(train_df['datetime'].dt.date)\n",
    "    test_dates = set(test_df['datetime'].dt.date)\n",
    "    if train_dates & test_dates:\n",
    "        raise ValueError(f\"Found {len(train_dates & test_dates)} overlapping dates between train and test sets!\")\n",
    "    \n",
    "    print(\"\\nClass Distributions:\")\n",
    "    print(\"Training Set:\")\n",
    "    print(train_df['PRECTOTCORR_binary'].value_counts(normalize=True))\n",
    "    print(\"\\nTesting Set:\")\n",
    "    print(test_df['PRECTOTCORR_binary'].value_counts(normalize=True))\n",
    "    \n",
    "    return train_df, test_df, features, 'PRECTOTCORR_binary'\n",
    "\n",
    "# Step 2: Balance and Scale Data\n",
    "def preprocess_and_balance_data(train_df, test_df, features, target):\n",
    "    # Initialize scaler and scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df[features])\n",
    "    test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(train_scaled, train_df[target])\n",
    "    \n",
    "    # Create balanced DataFrames\n",
    "    train_balanced = pd.DataFrame(X_resampled, columns=features)\n",
    "    train_balanced[target] = y_resampled\n",
    "    \n",
    "    test_processed = pd.DataFrame(test_scaled, columns=features, index=test_df.index)\n",
    "    test_processed[target] = test_df[target]\n",
    "    test_processed['DOY'] = test_df['DOY']  # Preserve for sequence generation\n",
    "    \n",
    "    print(\"\\nAfter SMOTE balancing:\")\n",
    "    print(train_balanced[target].value_counts())\n",
    "    \n",
    "    return train_balanced, test_processed, scaler\n",
    "\n",
    "# Step 3: Create Time Sequences\n",
    "def create_sequences(data, features, target, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[features].iloc[i:i+seq_length].values)\n",
    "        y.append(data[target].iloc[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 4: Build Transformer Model\n",
    "def build_transformer_model(seq_length, n_features):\n",
    "    inputs = Input(shape=(seq_length, n_features))\n",
    "    \n",
    "    # Positional Encoding\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "    positions = positions / tf.math.pow(10000, 2 * (tf.range(n_features, dtype=tf.float32) // 2) / n_features)\n",
    "    positions = tf.reshape(positions, (1, seq_length, n_features))\n",
    "    x = inputs + positions\n",
    "\n",
    "    # Transformer Block 1\n",
    "    attention_output = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "    attention_output = Dropout(0.3)(attention_output)\n",
    "    out1 = Add()([x, attention_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "    \n",
    "    # Feed Forward Network\n",
    "    ffn = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(out1)\n",
    "    ffn = Dropout(0.3)(ffn)\n",
    "    ffn = Dense(n_features, kernel_regularizer=l2(0.01))(ffn)\n",
    "    out2 = Add()([out1, ffn])\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "\n",
    "    # Transformer Block 2\n",
    "    attention_output = MultiHeadAttention(num_heads=8, key_dim=64)(out2, out2)\n",
    "    attention_output = Dropout(0.3)(attention_output)\n",
    "    out3 = Add()([out2, attention_output])\n",
    "    out3 = LayerNormalization(epsilon=1e-6)(out3)\n",
    "    \n",
    "    ffn = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(out3)\n",
    "    ffn = Dropout(0.3)(ffn)\n",
    "    ffn = Dense(n_features, kernel_regularizer=l2(0.01))(ffn)\n",
    "    out4 = Add()([out3, ffn])\n",
    "    out4 = LayerNormalization(epsilon=1e-6)(out4)\n",
    "\n",
    "    # Output Layer\n",
    "    x = Flatten()(out4)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', \n",
    "                        tf.keras.metrics.Precision(name='precision'),\n",
    "                        tf.keras.metrics.Recall(name='recall'),\n",
    "                        tf.keras.metrics.AUC(name='auc')])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Step 5: Train and Evaluate\n",
    "def train_and_evaluate(seq_length=15):\n",
    "    # Load and prepare data\n",
    "    train_df, test_df, features, target = load_and_prepare_data()\n",
    "    \n",
    "    # Preprocess and balance\n",
    "    train_balanced, test_processed, scaler = preprocess_and_balance_data(\n",
    "        train_df, test_df, features, target)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_balanced, features, target, seq_length)\n",
    "    X_test, y_test = create_sequences(test_processed, features, target, seq_length)\n",
    "    \n",
    "    print(f\"\\nFinal input shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_transformer_model(seq_length, len(features))\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                       epochs=200,\n",
    "                       batch_size=32,\n",
    "                       validation_split=0.2,\n",
    "                       callbacks=callbacks,\n",
    "                       verbose=1)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\n=== Test Set Evaluation ===\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1:        {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_prob):.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot results\n",
    "    plot_results(history, y_test, y_pred_prob, y_pred)\n",
    "    \n",
    "    return model, scaler, test_processed, features, target\n",
    "\n",
    "def plot_results(history, y_test, y_pred_prob, y_pred):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Training history\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.subplot(2, 2, 2)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc(fpr, tpr):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], ['No Rain', 'Rain'])\n",
    "    plt.yticks([0, 1], ['No Rain', 'Rain'])\n",
    "    \n",
    "    # Actual vs Predicted\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(y_test[:100], 'o-', label='Actual')\n",
    "    plt.plot(y_pred[:100], 'x--', label='Predicted')\n",
    "    plt.title('First 100 Test Samples')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Rain (1) or No Rain (0)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 6: Make Future Predictions\n",
    "def make_predictions(model, scaler, test_data, features, seq_length, days=7):\n",
    "    last_sequence = test_data[features].iloc[-seq_length:].values\n",
    "    predictions = []\n",
    "    last_doy = test_data['DOY'].iloc[-1]\n",
    "    \n",
    "    for _ in range(days):\n",
    "        x = last_sequence.reshape(1, seq_length, len(features))\n",
    "        pred_prob = model.predict(x, verbose=0)[0][0]\n",
    "        pred = 1 if pred_prob > 0.5 else 0\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        # Update sequence with predicted value\n",
    "        new_row = last_sequence[-1].copy()\n",
    "        new_row[-3] = pred_prob  # Update lag1 feature\n",
    "        last_sequence = np.vstack([last_sequence[1:], new_row])\n",
    "    \n",
    "    return predictions\n",
    "if __name__ == \"__main__\":\n",
    "    seq_length = 15\n",
    "    print(\"=== Hyderabad Rainfall Prediction Model ===\")\n",
    "    print(\"Initializing with rigorous temporal validation...\")\n",
    "    \n",
    "    try:\n",
    "        model, scaler, test_data, features, target = train_and_evaluate(seq_length)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = make_predictions(model, scaler, test_data, features, seq_length)\n",
    "        print(\"\\n=== 7-Day Rainfall Forecast for Hyderabad ===\")\n",
    "        for day, pred in enumerate(forecasts, 1):\n",
    "            print(f\"Day {day}: {'Rain' if pred == 1 else 'No Rain'}\")\n",
    "        \n",
    "        # Plot forecast\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(range(1, 8), forecasts, 'o-', color='blue')\n",
    "        plt.title('Hyderabad 7-Day Rainfall Forecast (2022 Data)')\n",
    "        plt.xlabel('Days Ahead')\n",
    "        plt.ylabel('Rain Prediction (1=Rain)')\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {str(e)}\")\n",
    "        print(\"Model training failed due to data validation issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6262efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Rainfall venv)",
   "language": "python",
   "name": "rainfall-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
